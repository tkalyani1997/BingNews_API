# Load the JSON file into a Spark DataFrame. The 'multiline' option is used to read multi-line JSON.
df = spark.read.option("multiline", "true").json("Files/bing-latest-news.json")

# Display the loaded DataFrame containing the JSON data.
display(df)

# Select the "value" column from the DataFrame which contains the actual news articles.
df = df.select("value")
display(df)

# Explode the "value" column, which is an array of JSON objects, into individual rows for each news article.
from pyspark.sql.functions import explode
df_exploded = df.select(explode(df["value"]).alias("json_object"))

# Display the exploded DataFrame where each row is a JSON object representing a news article.
display(df_exploded)

# Convert each row (which is a JSON object) into a list of JSON strings.
json_list = df_exploded.toJSON().collect()

# Print the second JSON string in the list for inspection.
print(json_list[1])

# Parse the second JSON string into a Python dictionary for inspection.
import json
new_json = json.loads(json_list[1])

# Print the parsed JSON dictionary.
print(new_json)

# Initialize empty lists to store relevant data from each JSON object.
title = []
description = []
category = []
url = []
image = []
provider = []
datepublished = []

# Iterate over each JSON string, parse it, and extract the required fields.
for json_str in json_list:
    try:
        article = json.loads(json_str)

        # Only process the article if both 'category' and 'image' fields are present.
        if article["json_object"].get("category") and article["json_object"].get("image", {}).get("thumbnail", {}).get("contentUrl"):
            
            # Extract the relevant fields and append them to their respective lists.
            title.append(article["json_object"]["name"])
            description.append(article["json_object"]["description"])
            category.append(article["json_object"]["category"])
            url.append(article["json_object"]["url"])
            image.append(article["json_object"]["image"]["thumbnail"]["contentUrl"])
            provider.append(article["json_object"]["provider"][0]['name'])
            datepublished.append(article["json_object"]["datePublished"])

    # Handle any exceptions and print an error message if a JSON object cannot be processed.
    except Exception as e:
        print(f"Error processing JSON object : {e}")

# Print the list of titles as an example of extracted data.
title

# Define a schema for the new DataFrame that will store the cleaned data.
from pyspark.sql.types import StructType, StructField, StringType

# Combine the lists into a list of tuples, each tuple representing one article.
data = list(zip(title, description, category, url, image, provider, datepublished))

# Define the schema for the cleaned DataFrame with column names and types.
schema = StructType([
    StructField("title", StringType(), True),
    StructField("description", StringType(), True),
    StructField("category", StringType(), True),
    StructField("url", StringType(), True),
    StructField("image", StringType(), True),
    StructField("provider", StringType(), True),
    StructField("datepublished", StringType(), True),
])

# Create a new DataFrame from the cleaned data using the defined schema.
df_cleaned = spark.createDataFrame(data, schema=schema)

# Display the cleaned DataFrame.
display(df_cleaned)

# Import functions to format the 'datepublished' column.
from pyspark.sql.functions import to_date, date_format

# Convert the 'datepublished' column to the 'dd-MM-yyyy' date format.
df_cleaned_final = df_cleaned.withColumn("datePublished", date_format(to_date("datepublished"), "dd-MM-yyyy"))

# Display the first 5 rows of the final cleaned DataFrame.
display(df_cleaned_final.limit(5))

# Save the cleaned DataFrame into a Delta table for future analysis.
df_cleaned_final.write.format("delta").saveAsTable("bing_lake_db.tbl_latest_news")

# Query the saved Delta table to fetch the first 1000 records.
df = spark.sql("SELECT * FROM bing_lake_db.tbl_latest_news LIMIT 1000")

# Display the queried data from the Delta table.
display(df)
